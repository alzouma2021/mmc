<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Node.js serverless functions on Red Hat OpenShift, Part 2: Debugging locally</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/dtR3hfYpuvA/nodejs-serverless-functions-red-hat-openshift-part-2-debugging-locally" /><author><name>Lucas Holmquist</name></author><id>cbd5d5fe-b00a-46d3-95e1-123856ec6c40</id><updated>2021-07-13T07:00:00Z</updated><published>2021-07-13T07:00:00Z</published><summary type="html">&lt;p&gt;Welcome back to our series on using serverless functions on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. The previous article introduced you to &lt;a href="https://developers.redhat.com/articles/2021/05/26/nodejs-serverless-functions-openshift-logging"&gt;how logging works in Node.js&lt;/a&gt; and how to customize what is logged in a Node.js function application. Now, we'll take a look at how to debug &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; function-based applications. Because debugging is a longer topic, we'll cover it in two parts. This article walks through how to set up and debug function applications locally with Visual Studio Code (VS Code). The next article will show you how to connect and debug function applications running in a container on a cluster.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: For an introduction to logging function-based applications, see &lt;em&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/26/nodejs-serverless-functions-openshift-logging"&gt;Node.js serverless functions on Red Hat OpenShift, Part 1: Logging&lt;/a&gt;&lt;/em&gt;. For an overview of Red Hat OpenShift Serverless Functions, see &lt;em&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions"&gt;Create your first serverless function with Red Hat OpenShift Serverless Functions&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To follow along with this article, you will need to install Node.js and download the example application from &lt;a href="https://github.com/nodeshift-blog-examples/debugging-with-functions"&gt;GitHub&lt;/a&gt;. We'll also use &lt;a href="https://code.visualstudio.com/"&gt;VS Code&lt;/a&gt; for its easy-to-use built-in debugger.&lt;/p&gt; &lt;p&gt;As with the &lt;a href="https://developers.redhat.com/articles/2021/05/26/nodejs-serverless-functions-openshift-logging" target="_blank"&gt;previous article&lt;/a&gt;, we scaffolded this function application with the &lt;code&gt;kn func&lt;/code&gt; command-line interface (CLI) tool. If you are not already familiar with it, you can learn more by reading &lt;a href="https://developers.redhat.com/blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions"&gt;&lt;em&gt;Create your first serverless function with Red Hat OpenShift Serverless Functions&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Setting up the function application in Visual Studio Code&lt;/h2&gt; &lt;p&gt;Use Git to clone the &lt;a href="https://github.com/nodeshift-blog-examples/debugging-with-functions"&gt;example repository&lt;/a&gt; and then open it up in VS Code. We can see that this Node.js function application is just like any other Node.js application, with an &lt;code&gt;index.js&lt;/code&gt; file where the main function logic is located.&lt;/p&gt; &lt;p&gt;Before we continue, let's put a breakpoint right around line 30, which is inside the &lt;code&gt;invoke&lt;/code&gt; function (see Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Adding a breakpoint to a serverless function in VS Code." data-entity-type="file" data-entity-uuid="4833664b-66aa-4a06-bf6c-dcffd720c020" src="https://developers.redhat.com/sites/default/files/inline-images/functions-debugging-set-break-point.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Adding a breakpoint to a serverless function in VS Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We are setting the breakpoint here because we want to be able to halt the execution of the function when it is called, and the &lt;code&gt;invoke&lt;/code&gt; function is the entry point generated by the &lt;code&gt;kn func&lt;/code&gt; CLI tool. This allows us to step through the code and inspect the different variables the function provides as the function executes.&lt;/p&gt; &lt;p&gt;Let's take a look at the &lt;code&gt;package.json&lt;/code&gt; file. We can see in the following code example that there are three npm scripts generated by the &lt;code&gt;kn func&lt;/code&gt; CLI tool: one for running, another for testing, and another one for debugging. This last script is the one we are interested in.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;"scripts": {     "test": "node test/unit.js &amp;&amp; node test/integration.js",     "local": "faas-js-runtime ./index.js",     "debug": "nodemon --inspect ./node_modules/faas-js-runtime/bin/cli.js ./index.js"   }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are a few things to note about this debug script. First, it uses &lt;a href="https://nodemon.io/"&gt;Nodemon&lt;/a&gt; to start the Node.js process. Nodemon will also detect any code changes and restart the Node.js process when the changes are saved.&lt;/p&gt; &lt;p&gt;The second is the &lt;code&gt;--inspect&lt;/code&gt; flag. This allows us to stop the Node.js process at any breakpoints we set. At the moment, we've only set one.&lt;/p&gt; &lt;p&gt;The last is that the script is called with the &lt;code&gt;faas-js-runtime&lt;/code&gt; CLI. This is a module that provides a Node.js framework for executing a function. The function listens for incoming HTTP requests at &lt;code&gt;localhost:8080&lt;/code&gt;. The incoming request can be a &lt;a href="https://cloudevents.io/"&gt;CloudEvent&lt;/a&gt; or just a simple HTTP GET request. To learn more about the &lt;code&gt;faas-js-runtime&lt;/code&gt;, check out the project on &lt;a href="https://github.com/boson-project/faas-js-runtime"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Debugging the function application&lt;/h2&gt; &lt;p&gt;Starting the debugging process is fairly simple. Select &lt;strong&gt;Start Debugging&lt;/strong&gt; from the &lt;strong&gt;Run&lt;/strong&gt; menu, as shown in Figure 2.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Screenshot showing the debug menu in VS Code." data-entity-type="file" data-entity-uuid="47475d74-466d-499a-a4a2-1d05239e0314" src="https://developers.redhat.com/sites/default/files/inline-images/functions-debugging-start-debugger-run-menu.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Starting the debug process in VS Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This initializes the Node.js process using the &lt;code&gt;--inspect&lt;/code&gt; flag and Nodemon. Once the process starts, your function runs at &lt;a href="http://localhost:8080"&gt;&lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/a&gt;. Navigating to this URL should activate the breakpoint that we set earlier (see Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="breakpoint active" data-entity-type="file" data-entity-uuid="f16f1291-fb1f-4950-bdc6-7b554a17cc21" src="https://developers.redhat.com/sites/default/files/inline-images/functions-debugging-breakpoint-stop-1.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: Activating the breakpoint in VS Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;From here, we can inspect any of the variables that are available to us. Functions are invoked with a &lt;code&gt;context&lt;/code&gt; object, which can be inspected easily using VS Code's variable inspector on the left-hand side of the interface (as shown in Figure 4). This object provides access to the incoming request information. You can get the HTTP request method, any query strings sent with the request, the headers, the HTTP version, or the request body. If the incoming request is a CloudEvent, the CloudEvent itself will also be found on the &lt;code&gt;context&lt;/code&gt; object.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Context object expanded in debug" data-entity-type="file" data-entity-uuid="35ad2c9b-fff2-409b-8102-737083bd5d2b" src="https://developers.redhat.com/sites/default/files/inline-images/debugging-functions-context-var.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4: Context object expanded in debug mode.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The request is a simple GET request. We can see from the variable inspector that it has no body or query params. As with most debugging tools, you can perform many debugging functions like stepping into and over a method, as well as just telling the process to continue executing.&lt;/p&gt; &lt;p&gt;Next, let's send a request to the function with a body. You can use this &lt;code&gt;curl&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -X POST -d '{"hello": "world"}' -H'Content-type: application/json' http://localhost:8080&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When the process stops this time, we can see that there is some data in the &lt;code&gt;context.body&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{   context: {     body: {       hello: “name”      }   } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the request was sent as a &lt;a href="https://cloudevents.io/"&gt;CloudEvent&lt;/a&gt;, this will help you easily inspect the request headers to learn more about it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -X POST -d '{"hello": "world"}' \   -H'Content-type: application/json' \   -H'Ce-id: 1' \   -H'Ce-source: cloud-event-example' \   -H'Ce-type: dev.knative.example' \   -H'Ce-specversion: 0.2' \   http://localhost:8080&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To learn more about this &lt;code&gt;context&lt;/code&gt; object and the parameters it provides to the function developer, &lt;a href="https://openshift-knative.github.io/docs/docs/functions/dev_guide/nodejs/context-obj-reference.html"&gt;check here&lt;/a&gt;. To learn more about CloudEvents, &lt;a href="https://cloudevents.io/"&gt;check here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article introduced you to debugging a Node.js serverless function application locally while you develop the function application. Stay tuned for the next part of this series, where we'll look at how to debug the function application while running inside a container on a Kubernetes cluster such as Red Hat OpenShift.&lt;/p&gt; &lt;p&gt;While you wait, you can read about the latest on &lt;a href="https://openshift-knative.github.io/docs/docs/functions/about-functions.html"&gt;OpenShift Serverless Functions&lt;/a&gt;. To learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/13/nodejs-serverless-functions-red-hat-openshift-part-2-debugging-locally" title="Node.js serverless functions on Red Hat OpenShift, Part 2: Debugging locally"&gt;Node.js serverless functions on Red Hat OpenShift, Part 2: Debugging locally&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/dtR3hfYpuvA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2021-07-13T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/13/nodejs-serverless-functions-red-hat-openshift-part-2-debugging-locally</feedburner:origLink></entry><entry><title type="html">Instantaneous Feedback Loop for DMN Authoring with DMN Runner</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/G6T7nfL6Dyc/instantaneous-feedback-loop-for-dmn-authoring-with-dmn-runner.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/07/instantaneous-feedback-loop-for-dmn-authoring-with-dmn-runner.html</id><updated>2021-07-12T05:00:00Z</updated><content type="html">We’ve been exploring ways to augment the developer experience for our editors on the KIE Tooling team. Today, I’m proud to announce the release of our DMN Runner on . We hope it will transform your DMN Authoring experience. Let’s see it in action: INSTANTANEOUS FEEDBACK LOOP An essential piece of a seamless authoring experience is an instantaneous feedback loop. A feedback loop on asset authoring happens when some portion of or all asset input is captured, analyzed, and used to provide insight to improve a future users’ authoring experience. Providing such instantaneous feedback is crucial to a fluid authoring experience. But what does this mean for DMN authoring? AUTOMATIC FORM GENERATION On every change of your DMN model, the DMN runner introspects it and automatically generates the input form for your DMN execution. The data that you input on it will be later used on the execution loop. AUTOMATIC DMN EVALUATION As soon as the DMN runner is connected to your dmn.new session, on every change of your DMN model, we will combine your DMN model and your form input and evaluate it on the DMN Engine. The awesome part of this workflow is that it is really fast, looking almost instantaneous. See it in action: AUTOMATIC DMN VALIDATION Another cool feature of DMN Runner is that together with DMN evaluation, we also validate your DMN models. Take a look at it: HOW TO START TO USE IT? It’s super simple. Go to and click on the ‘DMN Runner’ button and follow our wizard. The DMN runner is part of the KIE Tooling Extended Services Desktop application, and we provide binaries for all the platforms. After installing, dmn.new will automatically connect to your local environment and augment it, providing a much more fluid experience for your authoring! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/G6T7nfL6Dyc" height="1" width="1" alt=""/&gt;</content><dc:creator>Eder Ignatowicz</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/instantaneous-feedback-loop-for-dmn-authoring-with-dmn-runner.html</feedburner:origLink></entry><entry><title type="html">Kogito Tooling 0.11.0 Released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FE87raV3XMI/kogito-tooling-0-11-0-released.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/07/kogito-tooling-0-11-0-released.html</id><updated>2021-07-09T05:00:00Z</updated><content type="html">We have just launched a fresh new Kogito Tooling release! &#x1f389; On the 0.11.0 , we made a lot of improvements and bug fixes. We are also happy to announce that this release marks the first release of our DMN Runner on ! This post will give a quick overview of this . I hope you enjoy it! INSTANTANEOUS FEEDBACK LOOP FOR DMN AUTHORING WITH DMN RUNNER We’ve been exploring ways to augment the developer experience for our editors on the KIE Tooling team. Today, I’m proud to announce the release of our DMN Runner on dmn.new. We hope it will transform your DMN Authoring experience. Let’s see it in action: For this reason, we decided to start a new development stream for our BPMN, DMN, and Scenario Simulator editors called , freeing the way for Editors to continue evolving on both streams separately (Kogito and BC) without carrying the weight of the other. Soon we will write a blog post describing this new awesome feature. NEW FEATURES, FIXED ISSUES, AND IMPROVEMENTS We also made some new features, a lot of refactorings and improvements, with highlights to: * – Online Editor DMN Runner (First Iteration) * – The nodes should be created on top of the selected node in DMN editor * – BPMN Editor – Improve SVG generated ids * – [DMN Designer] Read-only mode – Connectors appear differently on read-only mode * – Implement integration tests for Save operation in BPMN editor in VSCode * – [SceSim Designer] HiDPI is not working as expected * – [DMN/BPMN] Sync kogito-editors-java with latest translations * – Kogito Tooling VS Code extensions Workspaces Trust * – Prevent different envelopes in the same window to interact with each other * – New elements should always be connected by their central magnetic point * – Stunner – Palette fixes &amp;amp; improvements * – Guided tour for invalid DMN models * – It’s not possible to save arrow edits * – SceSim Editor does not work in Eclipse Theia * – [BC included] [DMN/BPMN editor] Sometimes clicking outside doesn’t unselect nodes * – Stunner – Texts overlap toolboxes * – [BC Included] DMN editor removing edges for duplicate Decision Nodes on canvas * – Inconsistent results of integration tests across CI * – Stunner – Unknown Custom tasks in Designer makes Diagram Explorer empty * – Clear selection button doesn’t work on Testing Tools when use click property first time. * – Stunner – Editing text using Inline editor is shown over Properties panel or Expanded Palette * – Stunner – The order of Custom tasks in the palette is different with every process opening FURTHER READING/WATCHING We had some excellent blog posts on Kie Blog that I recommend you read: * , by Kirill Gaevskii; * , by Guilherme Carreiro; * , by Valentino Pellegrino; We also presented in some Kie Lives: * , by William Siqueira; * , by Guilherme Carreiro; * , by Guilherme Caponetto. THANK YOU TO EVERYONE INVOLVED! I would like to thank everyone involved with this release, from the excellent KIE Tooling Engineers to the lifesavers QEs and the UX people that help us look awesome! [kie] The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FE87raV3XMI" height="1" width="1" alt=""/&gt;</content><dc:creator>Eder Ignatowicz</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/kogito-tooling-0-11-0-released.html</feedburner:origLink></entry><entry><title type="html">RESTEasy 4.7.0.Final is now available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/PW_KlSk8v_M/" /><author><name /></author><id>https://resteasy.github.io/2021/07/08/resteasy-4.7.0.Final/</id><updated>2021-07-08T21:49:00Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/PW_KlSk8v_M" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/07/08/resteasy-4.7.0.Final/</feedburner:origLink></entry><entry><title>Troubleshooting application performance with Red Hat OpenShift metrics, Part 1: Requirements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/iURyZgi6LDM/troubleshooting-application-performance-red-hat-openshift-metrics-part-1" /><author><name>Pavel Macik</name></author><id>951504be-916c-414c-b971-9f2b68403bd1</id><updated>2021-07-08T07:00:00Z</updated><published>2021-07-08T07:00:00Z</published><summary type="html">&lt;p&gt;This is the first In a series of five articles showing how developers can use an extensive set of metrics offered by &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; to diagnose and fix performance problems. I'll describe a real-life success story where I did performance testing on the &lt;a href="https://developers.redhat.com/blog/2019/12/19/introducing-the-service-binding-operator"&gt;Service Binding Operator &lt;/a&gt; to get it accepted into the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. I'll describe the Service Binding Operator's performance challenges, how I planned my troubleshooting, and how I created and viewed the metrics.&lt;/p&gt; &lt;p&gt;This first article lays out the motivation for the whole effort, the Service Binding Operator's environment and testing setup, the requirements I had to meet to get the Service Binding Operator accepted into the Developer Sandbox, and the tooling made available by Developer Sandbox for performance testing.&lt;/p&gt; &lt;h2&gt;Motivation&lt;/h2&gt; &lt;p&gt;My team wanted to make the Service Binding Operator available on the Developer Sandbox for Red Hat OpenShift. Our goal was to use the operator in a demo and workshop titled &lt;a href="developer-sandbox/activities/connecting-to-your-managed-kafka-instance" target="_blank"&gt;Connecting to your Managed Kafka instance from the Developer Sandbox for Red Hat OpenShift&lt;/a&gt; at the April 2021 &lt;a href="https://events.summit.redhat.com/widget/redhat/sum21/sessioncatalog"&gt;Red Hat Summit&lt;/a&gt;. We also think this operator is useful to developers experimenting with the sandbox.&lt;/p&gt; &lt;p&gt;One of the requirements to get the Service Binding Operator accepted into the Developer Sandbox was to pass a performance evaluation. This evaluation would basically ensure that the operator wouldn't crash the Developer Sandbox while being invoked by a reasonable load of active users.&lt;/p&gt; &lt;h2&gt;Environment: The Developer Sandbox&lt;/h2&gt; &lt;p&gt;Technically, the Developer Sandbox is a couple of operators installed on an ordinary OpenShift cluster, which is an instance of the &lt;a href="https://www.openshift.com/products/dedicated/" target="_blank"&gt;Red Hat OpenShift Dedicated&lt;/a&gt; managed, cloud-based service. Developer Sandbox is scaled to support many concurrent users and their activity. For each developer (active user) registered on the sandbox, two namespaces are created to help the developer try out, play with, and learn about the OpenShift environment.&lt;/p&gt; &lt;p&gt;From the perspective of the Service Binding Operator, the Developer Sandbox is just a regular OpenShift instance. So, making the Service Binding Operator available to Developer Sandbox users is a simple matter of installing the operator into the underlying OpenShift cluster.&lt;/p&gt; &lt;p&gt;There are two ways to install an operator into the sandbox's cluster. The first way takes advantage of the &lt;code&gt;redhat-operators&lt;/code&gt; catalog source, which is available out of the box in OpenShift through the &lt;a href="https://docs.openshift.com/container-platform/4.5/operators/understanding/olm/olm-understanding-olm.html"&gt;Operator Lifecycle Manager&lt;/a&gt;. That catalog source hosts the official Red Hat releases of the Service Binding Operator.&lt;/p&gt; &lt;p&gt;The second installation method is specific to OpenShift Dedicated, which offers add-ons for Red Hat tools. But because Service Binding Operator is still a technology preview (not yet generally available), there is no OpenShift Dedicated add-on for it. So, we decided to go with the first option and install the Service Binding Operator via the in-cluster Operator Hub from the official catalog source.&lt;/p&gt; &lt;h2&gt;Developer Sandbox requirements and limitations&lt;/h2&gt; &lt;p&gt;The Developer Sandbox team specified several requirements that the operator had to meet to be accepted and installed on the production instance of Developer Sandbox. Some were operational, but the ones relevant to this series are related to performance.&lt;/p&gt; &lt;h3&gt;Operational requirements&lt;/h3&gt; &lt;p&gt;These requirements address the Service Binding Operator's integration into OpenShift Dedicated and the Developer Sandbox.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The operator must not require the creation of any additional namespaces other than its own.&lt;/li&gt; &lt;li&gt;It must be available on OpenShift Dedicated to run on OpenShift Dedicated clusters. (I'll show how to upload the Service Binding Operator in the next article in this series.)&lt;/li&gt; &lt;li&gt;It must be able to operate with the &lt;a href="https://github.com/redhat-developer/app-services-operator"&gt;Red Hat OpenShift Application Services&lt;/a&gt; Operator.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Performance requirements&lt;/h3&gt; &lt;p&gt;The remainder of this series focuses on what I did to meet the performance requirements for the Developer Sandbox. Essentially, we could have only one instance of the Service Binding Operator in the whole cluster, and that operator had to be able to handle a maximum of 3,000 users per cluster, as well as up to 10,000 namespaces. (Consider that Service Binding Operator requires a namespace, and that each user gets two more.)&lt;/p&gt; &lt;p&gt;As a result, a Developer Sandbox cluster consumes a lot of Kubernetes or OpenShift resources. The 3,000 users and up to 10,000 namespaces can lead to up to 100,000 role bindings, hundreds of thousands of secrets, and thousands of config maps, pods, deployments, build configs, and so on, per cluster. The operator must be able to handle that load without consuming too many compute resources, which would compromise cluster stability.&lt;/p&gt; &lt;h2&gt;Tooling for performance evaluations&lt;/h2&gt; &lt;p&gt;The Developer Sandbox team has its own &lt;a href="https://github.com/codeready-toolchain/toolchain-e2e/blob/master/setup/README.adoc" target="_blank"&gt;setup tool&lt;/a&gt;, which was originally used to test the Developer Sandbox itself. The tool was made available for other operator teams for conducting performance evaluations. The only prerequisite is to have access to an OpenShift cluster of the scale equivalent to what we need in production where the tested operator (the Service Binding Operator and Red Hat OpenShift Application Services Operator in our case) is installed and running. The tool can then do the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Install the sandbox on the OpenShift cluster.&lt;/li&gt; &lt;li&gt;Simulate a specified &lt;em&gt;x&lt;/em&gt; number of developers registering into the sandbox and simulate &lt;em&gt;y&lt;/em&gt; of the &lt;em&gt;x&lt;/em&gt; developers as active, creating workloads in their namespaces. For example, we could simulate a cluster where 3,000 users are registered and 1,000 of them are active.&lt;/li&gt; &lt;li&gt;Clean the cluster if necessary.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The tool creates a default set of workloads in one namespace of each active user and makes it possible to add custom workloads (typically specific for the tested operator usage) in the active users' simulations.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;This article has laid out the performance requirements for the Service Binding Operator to be to accepted to the Developer Sandbox for Red Hat OpenShift. The rest of this series documents the performance journey through the following tasks:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Provision the OpenShift cluster&lt;/li&gt; &lt;li&gt;Install the sandbox into the OpenShift cluster&lt;/li&gt; &lt;li&gt;Install Service Binding Operator and the Red Hat OpenShift Application Services Operator into the OpenShift cluster&lt;/li&gt; &lt;li&gt;Simulate active users "using the Service Binding Operator"&lt;/li&gt; &lt;li&gt;Collect runtime metrics from OpenShift&lt;/li&gt; &lt;li&gt;Extract and compute the test results&lt;/li&gt; &lt;li&gt;Compile a report&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Stay tuned for Part 2, where we will set up the testing environment and simulations.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1" title="Troubleshooting application performance with Red Hat OpenShift metrics, Part 1: Requirements"&gt;Troubleshooting application performance with Red Hat OpenShift metrics, Part 1: Requirements&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/iURyZgi6LDM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Pavel Macik</dc:creator><dc:date>2021-07-08T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1</feedburner:origLink></entry><entry><title type="html">Shopping recommendations in PMML.</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Koi2zA6yuvs/shopping-recommendations-in-pmml.html" /><author><name>Gabriele Cardosi</name></author><id>https://blog.kie.org/2021/07/shopping-recommendations-in-pmml.html</id><updated>2021-07-07T07:28:36Z</updated><content type="html">In previous posts ( and ) we had a glance at how a PMML engine has been implemented inside Drools/Kogito ecosystem. This time we will start looking at a concrete example of a recommendation engine based on top of PMML. The first part of this post will deal with the ML aspect of it, and the second part will talk of the actual usage of the generated model. So, let’s start… USE CASE DESCRIPTION The ABC Inc. wants to increase its sales. In the past, they already tried to suggest the product they would like to sell but without a good result. Recently, they heard about AI so they want to use a more advanced approach where the suggested product is not decided upfront but it is defined based on the behaviour of the users. The company sells three kinds of items: Books, PCs, and Cars, but usually the customers buy one type of item, seldom the others. So, the company wants to recommend items of the preferred type and not ones that have already been bought. DATA PREPARATION AND MODEL CREATION The code used to generate data and the PMML model is available at repository.  For simplicity, there are ten items for each type, and they are named Book-0, Book-1, Book-2 etc. The script is used to randomly generate a sample of 1000 customers that preferentially buy one kind of item, and possibly some other items of different types. A 30-dimensional array represents the purchased items, where 1 means a bought item. The first 10 elements are the Books, the next 10 elements are the Cars, and the last 10 elements are the PCs. BookCarPC0, 1,…,910, 11,…, 1920, 21, … 29 Generated data are stored inside   We have chosen to use a cluster model to represent the distribution of customers in the three main buyer groups (Books, Cars, PCs). To define, train and test the model, the environment has been used. It provides a ML environment bound to a GitHub repository. A quick and clear tutorial of its usage with Python may be found   For the model training, part of the generated data has been extracted.  This shows both data and graphical output: clusterpredictionbuyer group5191Car8372Book2080PC5251Car9780PC The last step is the actual dump of the generated model to PMML format, done with the library.  Here’s the generated TRUSTY-PMML PRIMER Trusty-PMML offers an easy-to-use API to evaluate models against given input. First of all, a reference to a model-specific PMMLRuntime has to be created. Available methods are defined inside interface. Some of them are meant to be used inside a KieServer container, with a pre-generated Kjar that contains the model already compiled to java classes.  Others simply require a working reference to the PMML file, in which case the model will be compiled in-memory the first time it is executed. The following snippet shows an example of the latter: ClassLoader classloader = Thread.currentThread().getContextClassLoader(); URL pmmlUrl = classloader.getResource(PMML_FILENAME); File pmmlFile = FileUtils.getFile(pmmlUrl.getFile()); PMMLRuntime pmmlRuntime = new PMMLRuntimeFactoryImpl().getPMMLRuntimeFromFile(pmmlFile); The next step requires the creation of a PMMLContext containing the input data to use for evaluation. The following snippet shows an example of that: static PMMLContext getPMMLContext(String modelName, Map&lt;String, Object&gt; inputData) { String correlationId = "CORRELATION_ID"; PMMLRequestDataBuilder pmmlRequestDataBuilder = new PMMLRequestDataBuilder(correlationId, modelName); for (Map.Entry&lt;String, Object&gt; entry : parameters.entrySet()) { Object pValue = entry.getValue(); Class class1 = pValue.getClass(); pmmlRequestDataBuilder.addParameter(entry.getKey(), pValue, class1); } PMMLRequestData pmmlRequestData = pmmlRequestDataBuilder.build(); return new PMMLContextImpl(pmmlRequestData); } The last step it is to actually retrieve the evaluation of the model inside a PMML4Result: PMML4Result result = pmmlRuntime.evaluate(modelName, pmmlContext); PMML4Result contains the status of the execution (resultCode), the  name of the target field (resultObjectName), and all the variables evaluated by the model. The following snippet shows how to retrieve the target results: String targetField = pmml4Result.getResultObjectName(); Object result = pmml4Result.getResultVariables().get(targetField); THE RECOMMENDER ENGINE The uses the “cluster_buyer_predictor.pmml” with the engine to predict the “buyer-group” of a randomly created customer. The pmml engine is responsible for making such predictions.  Based on that, the rest of the code will identify which items have not been already purchased by the customer, and will suggest one of them.  The object is a simple DTO that is initialized with randomly selected preferred type and bought items, stored as List. public class Customer { private final List&lt;String&gt; buyedItems; public Customer() { ItemType itemType = ItemType.byClusterId(new Random().nextInt(3)); buyedItems = mainBuyedItems(itemType); buyedItems.addAll(casualBuys(itemType)); } The loop in the creates five customers and retrieves the recommendation for each of them. public static void main(String[] args) { IntStream.range(0, 5).forEach(i -&gt; { Customer customer = new Customer(); logger.info("Customer {}", customer); String recommendation = getRecommendation(customer); logger.info("We recommend: {}", recommendation); }); } class is the core of the application. It invokes method to translate the List of bought items to a 30-dimensional array of integers (0 and 1): int[] buyedItems = Converter.getBuyedItemsIndexes(customer); Then, it calls method to retrieve the cluster the customer belongs to: int clusterId = PMMLUtils.getClusterId(buyedItems); Last, based on the cluster id and the already bought items, a recommendation is generated: private static String getRecommendation(ItemType itemType, List&lt;String&gt; buyedItems) { logger.info("getRecommendation {} {}", itemType, buyedItems); List&lt;String&gt; alreadyBuyed = buyedItems .stream() .filter(buyed -&gt; buyed.startsWith(itemType.getClusterName())) .collect(Collectors.toList()); if (alreadyBuyed.size() == 10) { return null; } return IntStream.range(0, 10) .mapToObj(i -&gt; itemType.getClusterName() + "-" + i) .filter(itemName -&gt; !alreadyBuyed.contains(itemName)) .findFirst() .orElse(null); } , on the other hand, is where the PMML model is actually instantiated and evaluated, so let’s dive deeper into it. To start with, a static block initialize the PMMLRuntime, reading the pmml file: static { ClassLoader classloader = Thread.currentThread().getContextClassLoader(); final URL pmmlUrl = classloader.getResource(PMML_FILENAME); File pmmlFile = FileUtils.getFile(pmmlUrl.getFile()); PMML_RUNTIME = new PMMLRuntimeFactoryImpl().getPMMLRuntimeFromFile(pmmlFile); } The getClusterId method convert the int[] to a Map; for each element in the array, a new entry is created, with the index of the element as key, and the value of the element (cast to double) as value: public static int getClusterId(int[] buyedItems) { logger.info("getClusterId {}", buyedItems); Map&lt;String, Object&gt; inputData = new HashMap&lt;&gt;(); for (int i = 0; i &lt; buyedItems.length ; i ++) { inputData.put(String.valueOf(i), (double) buyedItems[i]); } PMML4Result pmml4Result = evaluate(PMML_RUNTIME, inputData, MODEL_NAME); logger.info("pmml4Result {}", pmml4Result); String clusterIdName = (String) pmml4Result.getResultVariables().get(OUTPUT_FIELD); return Integer.parseInt(clusterIdName); } The evaluate method creates a PMMLContext out of the provided Map and returns the result of the evaluation, as PMML4Result : private static PMML4Result evaluate(final PMMLRuntime pmmlRuntime, final Map&lt;String, Object&gt; inputData, final String modelName) { logger.info("evaluate with PMMLRuntime {}", pmmlRuntime); final PMMLRequestData pmmlRequestData = getPMMLRequestData(modelName, inputData); final PMMLContext pmmlContext = new PMMLContextImpl(pmmlRequestData); return pmmlRuntime.evaluate(modelName, pmmlContext); } SUM UP In this post we have tackled a real-world recommendation scenario using the PMML and Trusty-PMML engine. First, we have created some sample data and trained a KMeans cluster model out of them. Then, we have provided a brief explanation on the basic Trusty-PMML API. Last, we have shown a bare-bone java project that, featuring the pmml engine, is able to provide reliable recommendations. But this is just the start of the journey. In the next posts we will see how to implement a cloud-native service that remotely provides the required predictions, and then… but let’s not spoil the surprise. Stay tuned! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Koi2zA6yuvs" height="1" width="1" alt=""/&gt;</content><dc:creator>Gabriele Cardosi</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/shopping-recommendations-in-pmml.html</feedburner:origLink></entry><entry><title>Deploy .NET applications on Red Hat OpenShift using Helm</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Xq7zriv-M6U/deploy-net-applications-red-hat-openshift-using-helm" /><author><name>Tom Deseyn</name></author><id>3192b6f7-ec34-479d-9843-9ead1aa63a3f</id><updated>2021-07-07T07:00:00Z</updated><published>2021-07-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://helm.sh/"&gt;Helm&lt;/a&gt; is a package manager for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. It provides an easy way to deploy a set of resources on a Kubernetes or &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; clusters. This article starts with a quick introduction to Helm. Then we'll use it to deploy a .NET application.&lt;/p&gt; &lt;h2&gt;Overview of Helm&lt;/h2&gt; &lt;p&gt;A Helm &lt;em&gt;chart&lt;/em&gt; describes a set of resources to deploy. The chart uses templates that can be configured on the command line or through a YAML file.&lt;/p&gt; &lt;p&gt;Charts can be distributed by hosting them on an HTTP server called a &lt;em&gt;chart repository&lt;/em&gt;. The &lt;code&gt;helm&lt;/code&gt; executable can fetch charts from the repository.&lt;/p&gt; &lt;p&gt;A Helm chart deployed on Kubernetes is called a &lt;em&gt;release&lt;/em&gt;. After you install the initial release, it can be updated. Each update has a corresponding release number. The release can also be rolled back to an earlier version.&lt;/p&gt; &lt;h2&gt;Helm prerequisites and setup&lt;/h2&gt; &lt;p&gt;To use Helm, you need the &lt;code&gt;helm&lt;/code&gt; binary and an OpenShift cluster.&lt;/p&gt; &lt;p&gt;You can download the binary from the &lt;a href="https://github.com/helm/helm/releases"&gt;GitHub release&lt;/a&gt; page. After you’ve extracted the tarball, add the directory containing the command to your PATH environment variable. You can check that the command works by executing &lt;code&gt;helm version&lt;/code&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm version version.BuildInfo{Version:"v3.5.4", GitCommit:"1b5edb69df3d3a08df77c9902dc17af864ff05d1", GitTreeState:"clean", GoVersion:"go1.15.11"}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use the &lt;code&gt;oc&lt;/code&gt; client command to log in to your OpenShift cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ oc login --token=xxx --server=https://yyy&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are not familiar with the OpenShift client, check out the &lt;a href="https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html"&gt;Getting started with the OpenShift CLI&lt;/a&gt; documentation.&lt;/p&gt; &lt;p&gt;For our example, we’ll use the .NET Core 3.1 and .NET 5.0 versions. You can check which versions are available globally using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ oc get -n openshift is dotnet --template='{{range .spec.tags}}{{.name}}{{"\n"}}{{end}}' 2.1 2.1-el7 2.1-ubi8 3.1 3.1-el7 3.1-ubi8 latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows that the .NET 5 version is not available on my cluster. Because I am not an administrator. I will import the images into my project namespace using the Bash shell script &lt;code&gt;install-imagestreams.sh&lt;/code&gt; from the &lt;a href="https://github.com/redhat-developer/s2i-dotnetcore"&gt;s2i-dotnetcore&lt;/a&gt; repository. The repository has a &lt;a href="https://github.com/redhat-developer/s2i-dotnetcore#windows"&gt;PowerShell script&lt;/a&gt;, too:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ wget https://raw.githubusercontent.com/redhat-developer/s2i-dotnetcore/master/install-imagestreams.sh $ chmod +x install-imagestreams.sh $ ./install-imagestreams.sh --os rhel&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When the shell script finishes, you can check the versions available in the project namespace by running the previous command without the &lt;code&gt;-n openshift&lt;/code&gt; argument:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ oc get is dotnet --template='{{range .spec.tags}}{{.name}}{{"\n"}}{{end}}' 2.1 2.1-el7 2.1-ubi8 3.1 3.1-el7 3.1-ubi8 5.0 5.0-ubi8 latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows that .NET 5.0 is available now.&lt;/p&gt; &lt;h2&gt;Using the .NET Helm chart&lt;/h2&gt; &lt;p&gt;First, we’ll configure Helm to fetch templates from the &lt;code&gt;redhat-helm-charts&lt;/code&gt; repository:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm repo add redhat-charts https://redhat-developer.github.io/redhat-helm-charts&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can see the available charts using the &lt;code&gt;helm search&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm search repo redhat-charts NAME CHART VERSION ... redhat-charts/dotnet 0.0.1 ... redhat-charts/nodejs 0.0.1 ... redhat-charts/quarkus 0.0.3 ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To get more information about the &lt;code&gt;dotnet&lt;/code&gt; chart, you can use the &lt;code&gt;helm inspect&lt;/code&gt; command. The &lt;code&gt;readme&lt;/code&gt; subcommand displays a description of the template. The &lt;code&gt;values&lt;/code&gt; subcommand shows the configuration values of the template:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm inspect readme redhat-charts/dotnet | less ... You must change the `build.uri`, `build.ref` and `build.startupProject` to refer to your own application. `build.imageStreamTag.name` must be set to match the .NET version used by your application. ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I showed here, from the output of the &lt;code&gt;readme&lt;/code&gt; subcommand, the line that describes the mandatory values to set.&lt;/p&gt; &lt;h3&gt;Deploy the .NET Core 3.1 application&lt;/h3&gt; &lt;p&gt;Let’s now use the chart to deploy the .NET Core 3.1 version of the &lt;a href="https://github.com/redhat-developer/s2i-dotnetcore-ex"&gt;s2i-dotnetcore-ex&lt;/a&gt; sample application:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm install mydotnetapp \ --set build.uri=https://github.com/redhat-developer/s2i-dotnetcore-ex \ --set build.ref=dotnetcore-3.1 \ --set build.startupProject=app \ --set build.imageStreamTag.name=dotnet:3.1 \ --set build.imageStreamTag.useReleaseNamespace=true \ redhat-charts/dotnet NAME: mydotnetapp LAST DEPLOYED: Thu May 20 10:22:14 2021 NAMESPACE: demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Your .NET app is building! To view the build logs, run: oc logs bc/mydotnetapp --follow Note that your Deployment will report "ErrImagePull" and "ImagePullBackOff" until the build is complete. Once the build is complete, your image will be automatically rolled out.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first argument (&lt;code&gt;mydotnetapp&lt;/code&gt;) is the name for our release. We’ve specified the Git repository (&lt;code&gt;build.uri&lt;/code&gt;), the Git branch name (&lt;code&gt;build.ref&lt;/code&gt;), and the location of the .NET project file in the repository (&lt;code&gt;build.startupProject&lt;/code&gt;). Using &lt;code&gt;build.imageStreamTag.name&lt;/code&gt;, we selected the &lt;a href="https://github.com/redhat-developer/s2i-dotnetcore"&gt;s2i-dotnetcore&lt;/a&gt; image for .NET 3.1 that is used to build and run our application.&lt;/p&gt; &lt;p&gt;You can visually follow the build's progress in the OpenShift web console, as shown in Figure 1.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dotnet_app.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dotnet_app.png?itok=tzWMW9mF" width="600" height="416" alt="The Resources tab of the OpenShift console shows the status of the application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Resources tab for the application in the OpenShift console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Update the release to .NET 5&lt;/h3&gt; &lt;p&gt;Using the &lt;code&gt;helm upgrade&lt;/code&gt; command, you can make changes to the release. Let’s change the release to the .NET 5 version.&lt;/p&gt; &lt;p&gt;As we saw during the prerequisites, my cluster didn’t include a .NET 5 image. To use the image that was installed into the project namespace, I’ve added the argument &lt;code&gt;--set build.imageStreamTag.useReleaseNamespace=true&lt;/code&gt; to the &lt;code&gt;upgrade&lt;/code&gt; subcommand:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm upgrade mydotnetapp \ --set build.ref=dotnet-5.0 \ --set build.imageStreamTag.name=dotnet:5.0 \ --set build.imageStreamTag.useReleaseNamespace=true \ redhat-charts/dotnet Release "mydotnetapp" has been upgraded. Happy Helming! NAME: mydotnetapp LAST DEPLOYED: Thu May 20 10:25:39 2021 NAMESPACE: demo STATUS: deployed REVISION: 2 TEST SUITE: None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Instead of specifying these values on the command line, you can read them from a YAML file. For example, the values could be placed in a &lt;code&gt;values.yaml&lt;/code&gt; file that lives in a &lt;code&gt;.helm&lt;/code&gt; folder next to the .NET &lt;code&gt;csproj&lt;/code&gt; project file. The &lt;code&gt;values.yaml&lt;/code&gt; file can be checked into version control to track changes and to make the file available to other developers deploying the application. For our current release, we’re using the following values:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;build: uri: https://github.com/redhat-developer/s2i-dotnetcore-ex startupProject: app ref: dotnet-5.0 imageStreamTag: name: dotnet:5.0 useReleaseNamespace: true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The filename can be passed to &lt;code&gt;helm&lt;/code&gt; using the &lt;code&gt;-f&lt;/code&gt; argument:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm upgrade mydotnetapp -f values.yaml redhat-charts/dotnet Release "mydotnetapp" has been upgraded. Happy Helming! NAME: mydotnetapp LAST DEPLOYED: Thu May 20 10:32:33 2021 NAMESPACE: demo STATUS: deployed REVISION: 3 TEST SUITE: None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Changes to a &lt;code&gt;BuildConfiguration&lt;/code&gt; do not automatically trigger a new build. You need to start it manually:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ oc start-build mydotnetapp&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;View and configure the Helm chart in OpenShift&lt;/h3&gt; &lt;p&gt;The OpenShift console recognizes Helm charts. Under the &lt;strong&gt;Helm&lt;/strong&gt; tab, you can navigate to your chart and see the three revisions you’ve deployed, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/webconsole_helm.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/webconsole_helm.png?itok=na7Y2rBx" width="600" height="224" alt="The application's Revision History tab shows three revisions, and shows which is currently deployed." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The application's revision history in the OpenShift console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Under the &lt;strong&gt;Actions&lt;/strong&gt; dropdown menu, you can remove the release completely or roll back to an earlier version. These operations can also be performed from the terminal using the &lt;code&gt;helm history&lt;/code&gt; and &lt;code&gt;helm rollback&lt;/code&gt; commands.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;dotnet&lt;/code&gt; Helm chart provides many configuration settings that make it usable for a wide range of .NET applications. These settings support adding probes, sidecar containers, and more. If you want to do something that is not supported by the chart, you can download the chart into your source repository and customize it. When running &lt;code&gt;helm install&lt;/code&gt; or &lt;code&gt;helm upgrade&lt;/code&gt;, you can point to the chart that lives with your sources:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm pull --untardir charts --untar redhat-charts/dotnet $ git add charts ... make some changes to the chart at charts/dotnet … $ helm upgrade mydotnetapp -f values.yaml charts/dotnet&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has illustrated the purpose and use of Helm. We went through the steps of deploying and updating a .NET application on OpenShift using the .NET helm chart from &lt;a href="https://github.com/redhat-developer/redhat-helm-charts"&gt;redhat-helm-charts&lt;/a&gt;. To learn more about Helm, you can read the &lt;a href="https://helm.sh/docs/"&gt;Helm documentation&lt;/a&gt;. To learn more about the .NET Helm chart, you can run the &lt;code&gt;helm inspect readme redhat-charts/dotnet&lt;/code&gt; command.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/07/deploy-net-applications-red-hat-openshift-using-helm" title="Deploy .NET applications on Red Hat OpenShift using Helm"&gt;Deploy .NET applications on Red Hat OpenShift using Helm&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Xq7zriv-M6U" height="1" width="1" alt=""/&gt;</summary><dc:creator>Tom Deseyn</dc:creator><dc:date>2021-07-07T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/07/deploy-net-applications-red-hat-openshift-using-helm</feedburner:origLink></entry><entry><title type="html">Quarkus 2.0.1.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ejBYiKf9gCU/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-2-0-1-final-released/</id><updated>2021-07-07T00:00:00Z</updated><content type="html">We just released Quarkus 2.0.1.Final, our first maintenance release on top of 2.0. It is a safe upgrade for anyone already using 2.0.0.Final. If you are not using 2.0 already, please refer to the 2.0 migration guide. Full changelog You can get the full changelog of 2.0.1.Final on GitHub. Come...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ejBYiKf9gCU" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-2-0-1-final-released/</feedburner:origLink></entry><entry><title type="html">Quarkus 2, RESTEasy 4.6 fixes and more</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/tH7dK2zn5qw/" /><author><name /></author><id>https://resteasy.github.io/2021/07/06/resteasy-4.6.2.Final/</id><updated>2021-07-06T00:45:00Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/tH7dK2zn5qw" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/07/06/resteasy-4.6.2.Final/</feedburner:origLink></entry><entry><title type="html">Apache Camel 3.11 What's New</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/gkaO8XYtwO0/apache-camel-311-whats-new.html" /><author><name>Claus Ibsen</name></author><id>http://feedproxy.google.com/~r/ApacheCamel/~3/DYxmwGm8nIw/apache-camel-311-whats-new.html</id><updated>2021-07-04T18:57:00Z</updated><content type="html">Apache Camel 3.11 has just been released. This is a LTS release which will be supported for 1 year with regular patch and security releases. This blog post first details the noteworthy changes since the last 3.10 release from last month. For readers that are upgrading from the last 3.7 LTS release then we have added a summary section that highlights all the important new features and changes (3.7 to 3.11). At first what did we do since the 3.10 release. SO WHAT'S IN THIS RELEASE SINCE 3.10 This release introduces a set of new features and noticeable improvements that we will cover in this blog post. KAMELETS Kamelets is a higher level building blocks that we keep innovating and improve over the coming releases. For Camel 3.11 we worked on making Kamelets universal across the various runtimes such as standalone, Karaf, Spring Boot, and Quarkus. We added a new camel-kamelet-main component that is intended for developers to try out or develop custom Kamelets. This module runs standalone which is intentional as we want to ensure Kamelets are not tied to a specific runtime (or the cloud on Kubernetes) but are truly universal in any environment where you can use Camel. You can find an example with camel-kamelet-main at The YAML DSL has improved error reporting when parsing to better report to Camel end users where the problem is. COMMON SOURCE TIMESTAMP We added a `getSourceTimestamp` API on `Message` to get hold of the timestamp from the source of the message. The idea is to have a common API across all the Camel components that has a timestamp of the event (such as JMS, Kafka, AWS, File/FTP etc). CLOUD COMPONENT The Camel AWS, Azure, and HuaweiCloud components have had various bug fixes and smaller improvements. QUARKUS This release is the baseline for Quarkus 2 support which is to follow shortly after this release with a new Camel Quarkus release. SPRING BOOT We have upgraded to latest Spring Boot 2.5.1 release. NO OSGI CODE IN MAIN PROJECT We had about six remaining Camel components which had some special OSGi Java source code. The OSGi code has been ported over to the Camel Karaf project. BETTER JAVA 16 SUPPORT Although Java 16 is not officially supported, we did improve a few Camel components to make them work with Java 16. The official support is Java 11 (primary) and Java 8 (secondary). NEW COMPONENTS This release has a number of new components, data formats and languages: * camel-huaweicloud-functiongraph - To call serverless functions on Huawei Cloud * camel-huaweicloud-iam - To securely manage users on Huawei Cloud * camel-kamelet-main - Main to run Kamelet standalone * camel-resourceresolver-github - Resource resolver to load files from GitHub UPGRADING Make sure to read the if you are upgrading from a previous Camel version. RELEASE NOTES You can find more information about this release in the , with a list of JIRA tickets resolved in the release. SUMMARY OF CHANGES SINCE THE LAST 3.7 LTS RELEASE It is 6 months since the last 3.7 LTS release, and here is a high level summary of the most significant changes we have done: * Optimized core (faster startup and quicker routing engine) * Modularized core (even smaller core) * Reduced Object Allocations (lower memory footprint)   * Reflection free (Native compilation friendly) * Optimized toD EIP for messaging based components * Better startup and shutdown logging * Java Flight Recorder * Routes loader (Java, XML, YAML, Groovy, JavaScript, and Kotlin) * YAML DSL * Kamelets * 17 new components * Support for Spring Boot 2.5 and Quarkus 2.0 There are many other great new features and improvements that you can find detailed in each of the Whats New blog posts: * * *&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/gkaO8XYtwO0" height="1" width="1" alt=""/&gt;</content><dc:creator>Claus Ibsen</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/ApacheCamel/~3/DYxmwGm8nIw/apache-camel-311-whats-new.html</feedburner:origLink></entry></feed>
